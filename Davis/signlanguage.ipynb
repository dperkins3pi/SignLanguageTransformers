{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVW97mLfWo9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a73304-80a3-47ed-e702-1b006fb49071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Mount Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- User-Defined Paths ---\n",
        "TEST_DATA_CSV = \"/content/drive/MyDrive/sign_language_dataset/small_dataset/test.csv\"\n",
        "TRAIN_DATA_CSV = \"/content/drive/MyDrive/sign_language_dataset/small_dataset/train.csv\"\n",
        "VAL_DATA_CSV = \"/content/drive/MyDrive/sign_language_dataset/small_dataset/val.csv\"\n",
        "JOINT_DIR = \"/content/drive/MyDrive/sign_language_dataset/small_dataset/joint_data\"\n",
        "\n",
        "\n",
        "NUM_FEATURES = 237\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q \"/content/drive/MyDrive/small_dataset.zip\" -d \"/content/drive/MyDrive/sign_language_dataset\""
      ],
      "metadata": {
        "id": "7BR74uPCcFM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Get Set of ALL Actual Files ---\n",
        "print(f\"Scanning {JOINT_DIR} for all existing files...\")\n",
        "actual_joint_files = set(os.listdir(JOINT_DIR))\n",
        "print(f\"Found {len(actual_joint_files)} actual files in {JOINT_DIR}.\")\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_DATA_CSV)\n",
        "val_df = pd.read_csv(VAL_DATA_CSV)\n",
        "test_df = pd.read_csv(TEST_DATA_CSV)\n",
        "\n",
        "# --- Helper Function to Clean ---\n",
        "def clean_manifest(df, actual_files_set, name):\n",
        "    print(f\"\\nCleaning {name} manifest...\")\n",
        "    original_size = len(df)\n",
        "\n",
        "    # 1. Create the expected joint filename for each row\n",
        "    def get_joint_filename(video_file_mp4):\n",
        "        return os.path.splitext(video_file_mp4)[0] + '.csv'\n",
        "\n",
        "    df['joint_filename'] = df['Video file'].apply(get_joint_filename)\n",
        "\n",
        "    # 2. Create a \"mask\" (True/False) for rows where the file exists\n",
        "    mask = df['joint_filename'].isin(actual_files_set)\n",
        "\n",
        "    # 3. Filter the DataFrame to keep only the \"True\" rows\n",
        "    cleaned_df = df[mask]\n",
        "\n",
        "    # 4. Report\n",
        "    cleaned_size = len(cleaned_df)\n",
        "    removed_count = original_size - cleaned_size\n",
        "    print(f\"Removed {removed_count} rows with missing joint files.\")\n",
        "    print(f\"Original {name} size: {original_size} | Cleaned {name} size: {cleaned_size}\")\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "cleaned_train_df = clean_manifest(train_df, actual_joint_files, \"training\")\n",
        "cleaned_val_df = clean_manifest(val_df, actual_joint_files, \"validation\")\n",
        "cleaned_test_df = clean_manifest(test_df, actual_joint_files, \"test\")\n",
        "\n",
        "# --- Re-Build the Label Map ---\n",
        "print(\"\\nRe-building label map from CLEANED training data...\")\n",
        "\n",
        "# Get a sorted list of all unique glosses (words)\n",
        "all_glosses = sorted(cleaned_train_df['Gloss'].unique())\n",
        "\n",
        "# Create the map and its reverse\n",
        "gloss_to_index_map = {gloss: i for i, gloss in enumerate(all_glosses)}\n",
        "index_to_gloss_map = {i: gloss for i, gloss in enumerate(all_glosses)}\n",
        "\n",
        "# This is our NEW, correct number of classes\n",
        "NUM_CLASSES = len(all_glosses)\n",
        "\n",
        "print(f\"Found {NUM_CLASSES} unique classes with data. (Original was 100).\")\n",
        "\n",
        "# --- Final Validation Filter ---\n",
        "# It's possible the validation set has a gloss that was completely\n",
        "# removed from the training set. We must filter those out too.\n",
        "original_val_size = len(cleaned_val_df)\n",
        "final_val_df = cleaned_val_df[cleaned_val_df['Gloss'].isin(gloss_to_index_map.keys())]\n",
        "removed_count = original_val_size - len(final_val_df)\n",
        "\n",
        "if removed_count > 0:\n",
        "    print(f\"Removed {removed_count} rows from validation set (classes not in train set).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDRWYfmF0C9t",
        "outputId": "b2838752-2975-41a0-dac9-65019d80707f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning /content/drive/MyDrive/sign_language_dataset/small_dataset/joint_data for all existing files...\n",
            "Found 3488 actual files in /content/drive/MyDrive/sign_language_dataset/small_dataset/joint_data.\n",
            "\n",
            "Cleaning training manifest...\n",
            "Removed 103 rows with missing joint files.\n",
            "Original training size: 1800 | Cleaned training size: 1697\n",
            "\n",
            "Cleaning validation manifest...\n",
            "Removed 19 rows with missing joint files.\n",
            "Original validation size: 365 | Cleaned validation size: 346\n",
            "\n",
            "Cleaning test manifest...\n",
            "Removed 68 rows with missing joint files.\n",
            "Original test size: 1286 | Cleaned test size: 1218\n",
            "\n",
            "Re-building label map from CLEANED training data...\n",
            "Found 94 unique classes with data. (Original was 100).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SignLanguageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset with:\n",
        "    1. Truncation (max_seq_length=120)\n",
        "    2. Per-Video Pose Normalization (Anchored to AVG FACE, fallback to AVG ALL)\n",
        "    3. Augmentations (Jitter, Scale, Temporal Dropout)\n",
        "    \"\"\"\n",
        "    def __init__(self, manifest_df, joint_dir, gloss_to_index_map, apply_augmentation=False):\n",
        "        self.manifest = manifest_df\n",
        "        self.joint_dir = joint_dir\n",
        "        self.gloss_to_index_map = gloss_to_index_map\n",
        "        self.apply_augmentation = apply_augmentation\n",
        "\n",
        "        self.num_features = 237 # 79 joints * 3 coords\n",
        "        self.num_joints = 79\n",
        "        self.max_seq_length = 120\n",
        "\n",
        "        # Hand 0 (21 joints) + Hand 1 (21 joints) = 42\n",
        "        # Face joints are indices 42 through 78\n",
        "        self.face_joints_start_index = 42\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.manifest)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Get row\n",
        "        row = self.manifest.iloc[idx].copy()\n",
        "        video_filename_mp4 = row['Video file']\n",
        "        gloss = row['Gloss']\n",
        "\n",
        "        # 2. Construct path\n",
        "        joint_filename = os.path.splitext(video_filename_mp4)[0] + '.csv'\n",
        "        joint_filepath = os.path.join(self.joint_dir, joint_filename)\n",
        "\n",
        "        # 3. Load data\n",
        "        try:\n",
        "            pose_data_df = pd.read_csv(joint_filepath)\n",
        "            pose_sequence = pose_data_df.drop(columns=['frame']).values\n",
        "            if pose_sequence.shape[0] == 0:\n",
        "                return torch.zeros((10, self.num_features)), torch.tensor(0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {joint_filepath}: {e}\")\n",
        "            return torch.zeros((10, self.num_features)), torch.tensor(0)\n",
        "\n",
        "        # 3b. Truncate\n",
        "        if pose_sequence.shape[0] > self.max_seq_length:\n",
        "            pose_sequence = pose_sequence[:self.max_seq_length]\n",
        "\n",
        "        # 4. --- (NEW) Robust Per-Video Normalization (Face COM) ---\n",
        "\n",
        "        # Reshape to (num_frames, 79 joints, 3 coords)\n",
        "        joints = pose_sequence.reshape(-1, self.num_joints, 3)\n",
        "\n",
        "        # --- Plan A: Try to find the center of the face ---\n",
        "\n",
        "        # Get all face joints, shape (num_frames, 37, 3)\n",
        "        all_face_joints = joints[:, self.face_joints_start_index:, :]\n",
        "\n",
        "        # Flatten to find all valid points across the whole video\n",
        "        all_face_coords = all_face_joints.reshape(-1, 3)\n",
        "\n",
        "        # Filter for non-zero coordinates (valid points)\n",
        "        valid_face_coords = all_face_coords[np.sum(np.abs(all_face_coords), axis=1) > 1e-6]\n",
        "\n",
        "        if len(valid_face_coords) > 0:\n",
        "            # Plan A success: Use the average of all valid face points\n",
        "            reference_center = np.mean(valid_face_coords, axis=0)\n",
        "        else:\n",
        "            # --- Plan B: Fallback to center of ALL joints ---\n",
        "            # (This handles videos where the face is 100% occluded but hands are visible)\n",
        "\n",
        "            # Flatten all joints\n",
        "            all_joint_coords = joints.reshape(-1, 3)\n",
        "\n",
        "            # Filter for non-zero coordinates\n",
        "            valid_joint_coords = all_joint_coords[np.sum(np.abs(all_joint_coords), axis=1) > 1e-6]\n",
        "\n",
        "            if len(valid_joint_coords) > 0:\n",
        "                # Plan B success: Use the average of all valid points\n",
        "                reference_center = np.mean(valid_joint_coords, axis=0)\n",
        "            else:\n",
        "                # --- Final Fallback: Video is completely empty ---\n",
        "                reference_center = np.array([0.0, 0.0, 0.0])\n",
        "                print(\"NO FACE OR REFERENCE\")\n",
        "\n",
        "        # 5. Center and Scale the entire sequence\n",
        "        # Subtract the reference center from ALL joints in ALL frames\n",
        "        centered_joints_sequence = joints - reference_center\n",
        "\n",
        "        # Find the global max deviation (Scale)\n",
        "        reference_scale = np.max(np.abs(centered_joints_sequence))\n",
        "\n",
        "        if reference_scale == 0: # Avoid division by zero\n",
        "            reference_scale = 1.0\n",
        "\n",
        "        # Apply scale\n",
        "        scaled_joints_sequence = centered_joints_sequence / reference_scale\n",
        "\n",
        "        # 6. Flatten back to (num_frames, 237)\n",
        "        pose_sequence = scaled_joints_sequence.reshape(-1, self.num_features)\n",
        "        # ---------------------------------------------\n",
        "\n",
        "        # 7. Data Augmentation\n",
        "        if self.apply_augmentation:\n",
        "            noise = np.random.normal(0, 0.001, pose_sequence.shape)\n",
        "            pose_sequence = pose_sequence + noise\n",
        "\n",
        "            scale_factor = np.random.uniform(0.9, 1.1)\n",
        "            pose_sequence = pose_sequence * scale_factor\n",
        "\n",
        "            frame_dropout_prob = 0.1\n",
        "            mask = np.random.rand(pose_sequence.shape[0], 1) > frame_dropout_prob\n",
        "            pose_sequence = pose_sequence * mask\n",
        "\n",
        "        # 8. Convert to PyTorch tensors\n",
        "        pose_tensor = torch.tensor(pose_sequence, dtype=torch.float32)\n",
        "        label_index = self.gloss_to_index_map[gloss]\n",
        "        label_tensor = torch.tensor(label_index, dtype=torch.long)\n",
        "\n",
        "        return pose_tensor, label_tensor"
      ],
      "metadata": {
        "id": "U2px3Go26Qx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle padding for sequences of different lengths.\n",
        "    \"\"\"\n",
        "    sequences = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    return padded_sequences, labels, lengths"
      ],
      "metadata": {
        "id": "Nrc_Vl056obM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 450\n",
        "\n",
        "# --- Create Datasets ---\n",
        "# We pass the pre-cleaned DataFrames directly\n",
        "train_dataset = SignLanguageDataset(\n",
        "    cleaned_train_df,\n",
        "    JOINT_DIR,\n",
        "    gloss_to_index_map,\n",
        "    apply_augmentation=True\n",
        ")\n",
        "\n",
        "val_dataset = SignLanguageDataset(\n",
        "    final_val_df,\n",
        "    JOINT_DIR,\n",
        "    gloss_to_index_map,\n",
        "    apply_augmentation=False\n",
        ")\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"\\nDataLoaders created successfully with CLEANED data.\")\n",
        "print(f\"Total Training samples: {len(train_dataset)}\")\n",
        "print(f\"Total Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of classes to predict: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-WjVIwP6ugR",
        "outputId": "53532946-f226-4a5a-fa63-63a3965fe5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataLoaders created successfully with CLEANED data.\n",
            "Total Training samples: 1697\n",
            "Total Validation samples: 346\n",
            "Number of classes to predict: 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(PoseLSTM, self).__init__()\n",
        "\n",
        "        # Make the LSTM bidirectional\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.3 if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fc_head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_size * 2, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (Batch_Size, Seq_Length, Input_Size)\n",
        "\n",
        "        lstm_out, (hidden_state, cell_state) = self.lstm(x)\n",
        "\n",
        "        # We need to combine the final hidden states from both directions\n",
        "        # hidden_state shape: (Num_Layers * 2, Batch_Size, Hidden_Size)\n",
        "\n",
        "        # Get the last layer's hidden state (forward)\n",
        "        fwd_hidden = hidden_state[-2, :, :] # (Batch_Size, Hidden_Size)\n",
        "        # Get the last layer's hidden state (backward)\n",
        "        bwd_hidden = hidden_state[-1, :, :] # (Batch_Size, Hidden_Size)\n",
        "\n",
        "        # Concatenate them\n",
        "        combined_hidden = torch.cat((fwd_hidden, bwd_hidden), dim=1)\n",
        "        # combined_hidden shape: (Batch_Size, Hidden_Size * 2)\n",
        "\n",
        "        # Pass through our new classifier head\n",
        "        output = self.fc_head(combined_hidden)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "FMIAtegl6zsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Stops training when validation loss hasn't improved for a given 'patience'.\"\"\"\n",
        "    def __init__(self, patience=20, verbose=True, delta=0, path='best_model.pth', trace_func=print):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, epoch):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        if epoch < 150:\n",
        "          return\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "Qz0eKO1LbM73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topk_accuracy(output, target, k=5):\n",
        "    \"\"\"\n",
        "    Computes the accuracy over the k top predictions for the specified values of k.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "        # Get the top k indices\n",
        "        _, pred = output.topk(k, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        # Check if the true label is in the top k predictions\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        # Sum up correct predictions\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        # Return percentage\n",
        "        return correct_k.mul_(100.0 / batch_size).item()"
      ],
      "metadata": {
        "id": "SIfugsypXBEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# --- Check for GPU ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 500\n",
        "GRAD_CLIP_VALUE = 1.0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "train_top5_accs = []\n",
        "val_top5_accs = []\n",
        "\n",
        "\n",
        "model = PoseLSTM(NUM_FEATURES, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=35)\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/face_global_norm_sign_language_model.pth'\n",
        "early_stopping = EarlyStopping(patience=75, verbose=True, path=model_save_path)\n",
        "\n",
        "# --- (Initialize TensorBoard) ---\n",
        "writer = SummaryWriter('runs/my_bidirectional_lstm_v2')\n",
        "\n",
        "print(\"Starting training with Early Stopping and LR Scheduler...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    correct_train_top5 = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for padded_sequences, labels, lengths in train_loader:\n",
        "        padded_sequences = padded_sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(padded_sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * padded_sequences.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        batch_top5_acc = calculate_topk_accuracy(outputs, labels, k=5)\n",
        "        correct_train_top5 += (batch_top5_acc * labels.size(0)) / 100.0\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = 100 * correct_train / total_train\n",
        "    epoch_top5_acc = 100 * correct_train_top5 / total_train\n",
        "\n",
        "    # --- Validation Loop ---\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    correct_val_top5 = 0\n",
        "    total_val = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_seq, val_labels, val_lengths in val_loader:\n",
        "            val_seq = val_seq.to(device)\n",
        "            val_labels = val_labels.to(device)\n",
        "\n",
        "            val_outputs = model(val_seq)\n",
        "            val_loss = criterion(val_outputs, val_labels)\n",
        "            val_running_loss += val_loss.item() * val_seq.size(0)\n",
        "\n",
        "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "            total_val += val_labels.size(0)\n",
        "            correct_val += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "            batch_val_top5 = calculate_topk_accuracy(val_outputs, val_labels, k=5)\n",
        "            correct_val_top5 += (batch_val_top5 * val_labels.size(0)) / 100.0\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_acc = 100 * correct_val / total_val\n",
        "    epoch_val_top5_acc = 100 * correct_val_top5 / total_val\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {epoch_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Train Acc: {epoch_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    train_accs.append(epoch_acc)\n",
        "    val_accs.append(epoch_val_acc)\n",
        "\n",
        "    train_top5_accs.append(epoch_top5_acc)\n",
        "    val_top5_accs.append(epoch_val_top5_acc)\n",
        "\n",
        "\n",
        "    # 1. Give the scheduler the validation loss\n",
        "    scheduler.step(epoch_val_loss)\n",
        "\n",
        "    # 2. Give Early Stopping the validation loss and model\n",
        "    early_stopping(epoch_val_loss, model, epoch)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    # --- Log to TensorBoard ---\n",
        "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
        "    writer.add_scalar('Loss/val', epoch_val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
        "    writer.add_scalar('Accuracy/val', epoch_val_acc, epoch)\n",
        "    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "\n",
        "print(\"Finished Training!\")\n",
        "writer.close()\n",
        "\n",
        "print(f\"Loading best model from: {model_save_path}\")\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "print(\"Best model loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyDfH-Lo67st",
        "outputId": "90e7862f-8441-4bc7-b453-f05b843a82ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Starting training with Early Stopping and LR Scheduler...\n",
            "Epoch 1/500 | Train Loss: 4.5441 | Val Loss: 4.5403 | Train Acc: 0.94% | Val Acc: 0.58%\n",
            "Epoch 2/500 | Train Loss: 4.5397 | Val Loss: 4.5369 | Train Acc: 1.41% | Val Acc: 2.02%\n",
            "Epoch 3/500 | Train Loss: 4.5362 | Val Loss: 4.5327 | Train Acc: 1.94% | Val Acc: 2.60%\n",
            "Epoch 4/500 | Train Loss: 4.5321 | Val Loss: 4.5269 | Train Acc: 2.36% | Val Acc: 2.60%\n",
            "Epoch 5/500 | Train Loss: 4.5265 | Val Loss: 4.5179 | Train Acc: 2.77% | Val Acc: 3.76%\n",
            "Epoch 6/500 | Train Loss: 4.5156 | Val Loss: 4.5023 | Train Acc: 2.77% | Val Acc: 3.47%\n",
            "Epoch 7/500 | Train Loss: 4.4979 | Val Loss: 4.4722 | Train Acc: 2.95% | Val Acc: 3.47%\n",
            "Epoch 8/500 | Train Loss: 4.4653 | Val Loss: 4.4188 | Train Acc: 2.24% | Val Acc: 2.89%\n",
            "Epoch 9/500 | Train Loss: 4.4130 | Val Loss: 4.3647 | Train Acc: 2.59% | Val Acc: 2.89%\n",
            "Epoch 10/500 | Train Loss: 4.3661 | Val Loss: 4.3141 | Train Acc: 2.24% | Val Acc: 2.60%\n",
            "Epoch 11/500 | Train Loss: 4.3110 | Val Loss: 4.2699 | Train Acc: 2.77% | Val Acc: 2.89%\n",
            "Epoch 12/500 | Train Loss: 4.2684 | Val Loss: 4.2326 | Train Acc: 3.12% | Val Acc: 1.73%\n",
            "Epoch 13/500 | Train Loss: 4.2239 | Val Loss: 4.1885 | Train Acc: 3.77% | Val Acc: 2.60%\n",
            "Epoch 14/500 | Train Loss: 4.1815 | Val Loss: 4.1372 | Train Acc: 3.54% | Val Acc: 2.89%\n",
            "Epoch 15/500 | Train Loss: 4.1384 | Val Loss: 4.1137 | Train Acc: 4.07% | Val Acc: 3.18%\n",
            "Epoch 16/500 | Train Loss: 4.1053 | Val Loss: 4.0690 | Train Acc: 3.36% | Val Acc: 3.76%\n",
            "Epoch 17/500 | Train Loss: 4.0612 | Val Loss: 4.0415 | Train Acc: 3.83% | Val Acc: 4.05%\n",
            "Epoch 18/500 | Train Loss: 4.0353 | Val Loss: 3.9909 | Train Acc: 4.60% | Val Acc: 5.20%\n",
            "Epoch 19/500 | Train Loss: 4.0023 | Val Loss: 3.9685 | Train Acc: 4.89% | Val Acc: 5.20%\n",
            "Epoch 20/500 | Train Loss: 3.9591 | Val Loss: 3.9314 | Train Acc: 6.42% | Val Acc: 5.20%\n",
            "Epoch 21/500 | Train Loss: 3.9353 | Val Loss: 3.8924 | Train Acc: 6.95% | Val Acc: 6.94%\n",
            "Epoch 22/500 | Train Loss: 3.9016 | Val Loss: 3.8731 | Train Acc: 5.95% | Val Acc: 6.65%\n",
            "Epoch 23/500 | Train Loss: 3.8701 | Val Loss: 3.8281 | Train Acc: 6.07% | Val Acc: 7.80%\n",
            "Epoch 24/500 | Train Loss: 3.8380 | Val Loss: 3.8243 | Train Acc: 7.48% | Val Acc: 6.94%\n",
            "Epoch 25/500 | Train Loss: 3.7779 | Val Loss: 3.7599 | Train Acc: 9.25% | Val Acc: 8.67%\n",
            "Epoch 26/500 | Train Loss: 3.7503 | Val Loss: 3.7308 | Train Acc: 9.55% | Val Acc: 9.25%\n",
            "Epoch 27/500 | Train Loss: 3.7294 | Val Loss: 3.7298 | Train Acc: 8.49% | Val Acc: 10.40%\n",
            "Epoch 28/500 | Train Loss: 3.6888 | Val Loss: 3.6920 | Train Acc: 11.90% | Val Acc: 10.69%\n",
            "Epoch 29/500 | Train Loss: 3.6646 | Val Loss: 3.6383 | Train Acc: 10.78% | Val Acc: 12.43%\n",
            "Epoch 30/500 | Train Loss: 3.6156 | Val Loss: 3.6443 | Train Acc: 12.26% | Val Acc: 12.14%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 31/500 | Train Loss: 3.6045 | Val Loss: 3.6257 | Train Acc: 11.67% | Val Acc: 11.85%\n",
            "Epoch 32/500 | Train Loss: 3.5699 | Val Loss: 3.5863 | Train Acc: 14.02% | Val Acc: 14.16%\n",
            "Epoch 33/500 | Train Loss: 3.5394 | Val Loss: 3.5569 | Train Acc: 14.26% | Val Acc: 14.16%\n",
            "Epoch 34/500 | Train Loss: 3.5163 | Val Loss: 3.5406 | Train Acc: 13.20% | Val Acc: 14.16%\n",
            "Epoch 35/500 | Train Loss: 3.4787 | Val Loss: 3.5288 | Train Acc: 14.56% | Val Acc: 13.87%\n",
            "Epoch 36/500 | Train Loss: 3.4555 | Val Loss: 3.4983 | Train Acc: 14.73% | Val Acc: 15.61%\n",
            "Epoch 37/500 | Train Loss: 3.4307 | Val Loss: 3.4805 | Train Acc: 16.09% | Val Acc: 15.90%\n",
            "Epoch 38/500 | Train Loss: 3.4033 | Val Loss: 3.4600 | Train Acc: 14.61% | Val Acc: 16.47%\n",
            "Epoch 39/500 | Train Loss: 3.3831 | Val Loss: 3.4416 | Train Acc: 14.56% | Val Acc: 15.32%\n",
            "Epoch 40/500 | Train Loss: 3.3721 | Val Loss: 3.4260 | Train Acc: 17.21% | Val Acc: 16.47%\n",
            "Epoch 41/500 | Train Loss: 3.3304 | Val Loss: 3.4409 | Train Acc: 16.91% | Val Acc: 12.72%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 42/500 | Train Loss: 3.3269 | Val Loss: 3.3800 | Train Acc: 17.86% | Val Acc: 18.79%\n",
            "Epoch 43/500 | Train Loss: 3.3025 | Val Loss: 3.3948 | Train Acc: 17.91% | Val Acc: 15.90%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 44/500 | Train Loss: 3.2770 | Val Loss: 3.3678 | Train Acc: 18.09% | Val Acc: 16.76%\n",
            "Epoch 45/500 | Train Loss: 3.2574 | Val Loss: 3.3624 | Train Acc: 17.86% | Val Acc: 16.76%\n",
            "Epoch 46/500 | Train Loss: 3.2036 | Val Loss: 3.3363 | Train Acc: 20.45% | Val Acc: 18.50%\n",
            "Epoch 47/500 | Train Loss: 3.2034 | Val Loss: 3.3108 | Train Acc: 20.39% | Val Acc: 19.36%\n",
            "Epoch 48/500 | Train Loss: 3.1873 | Val Loss: 3.3059 | Train Acc: 18.80% | Val Acc: 19.36%\n",
            "Epoch 49/500 | Train Loss: 3.1942 | Val Loss: 3.2941 | Train Acc: 20.51% | Val Acc: 19.65%\n",
            "Epoch 50/500 | Train Loss: 3.1612 | Val Loss: 3.2844 | Train Acc: 20.39% | Val Acc: 21.39%\n",
            "Epoch 51/500 | Train Loss: 3.1354 | Val Loss: 3.3086 | Train Acc: 21.69% | Val Acc: 16.76%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 52/500 | Train Loss: 3.1188 | Val Loss: 3.2388 | Train Acc: 22.75% | Val Acc: 20.81%\n",
            "Epoch 53/500 | Train Loss: 3.1100 | Val Loss: 3.2695 | Train Acc: 21.80% | Val Acc: 18.21%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 54/500 | Train Loss: 3.0750 | Val Loss: 3.2149 | Train Acc: 22.27% | Val Acc: 20.23%\n",
            "Epoch 55/500 | Train Loss: 3.0570 | Val Loss: 3.2207 | Train Acc: 23.51% | Val Acc: 20.23%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 56/500 | Train Loss: 3.0586 | Val Loss: 3.2153 | Train Acc: 22.63% | Val Acc: 20.52%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 57/500 | Train Loss: 3.0333 | Val Loss: 3.1847 | Train Acc: 23.81% | Val Acc: 20.52%\n",
            "Epoch 58/500 | Train Loss: 3.0177 | Val Loss: 3.2181 | Train Acc: 24.63% | Val Acc: 18.21%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 59/500 | Train Loss: 2.9898 | Val Loss: 3.1695 | Train Acc: 24.63% | Val Acc: 19.65%\n",
            "Epoch 60/500 | Train Loss: 2.9923 | Val Loss: 3.1721 | Train Acc: 24.75% | Val Acc: 21.10%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 61/500 | Train Loss: 2.9736 | Val Loss: 3.1804 | Train Acc: 26.40% | Val Acc: 20.23%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 62/500 | Train Loss: 2.9387 | Val Loss: 3.1488 | Train Acc: 27.99% | Val Acc: 20.23%\n",
            "Epoch 63/500 | Train Loss: 2.9167 | Val Loss: 3.1211 | Train Acc: 27.70% | Val Acc: 20.81%\n",
            "Epoch 64/500 | Train Loss: 2.8917 | Val Loss: 3.0887 | Train Acc: 27.81% | Val Acc: 22.25%\n",
            "Epoch 65/500 | Train Loss: 2.8973 | Val Loss: 3.1136 | Train Acc: 27.70% | Val Acc: 19.65%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 66/500 | Train Loss: 2.8855 | Val Loss: 3.1412 | Train Acc: 26.75% | Val Acc: 19.36%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 67/500 | Train Loss: 2.8649 | Val Loss: 3.0671 | Train Acc: 27.40% | Val Acc: 21.10%\n",
            "Epoch 68/500 | Train Loss: 2.8407 | Val Loss: 3.1181 | Train Acc: 29.70% | Val Acc: 18.79%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 69/500 | Train Loss: 2.8345 | Val Loss: 3.0711 | Train Acc: 29.17% | Val Acc: 21.39%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 70/500 | Train Loss: 2.8135 | Val Loss: 3.0665 | Train Acc: 30.17% | Val Acc: 20.52%\n",
            "Epoch 71/500 | Train Loss: 2.7975 | Val Loss: 3.0626 | Train Acc: 32.70% | Val Acc: 19.65%\n",
            "Epoch 72/500 | Train Loss: 2.7670 | Val Loss: 3.0992 | Train Acc: 30.88% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 73/500 | Train Loss: 2.7742 | Val Loss: 3.0464 | Train Acc: 29.82% | Val Acc: 20.23%\n",
            "Epoch 74/500 | Train Loss: 2.7558 | Val Loss: 3.1049 | Train Acc: 31.59% | Val Acc: 20.23%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 75/500 | Train Loss: 2.7268 | Val Loss: 3.0408 | Train Acc: 31.88% | Val Acc: 22.54%\n",
            "Epoch 76/500 | Train Loss: 2.7082 | Val Loss: 3.0095 | Train Acc: 32.12% | Val Acc: 22.25%\n",
            "Epoch 77/500 | Train Loss: 2.6866 | Val Loss: 3.0589 | Train Acc: 33.24% | Val Acc: 19.94%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 78/500 | Train Loss: 2.6817 | Val Loss: 3.0339 | Train Acc: 34.24% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 79/500 | Train Loss: 2.6451 | Val Loss: 2.9869 | Train Acc: 35.77% | Val Acc: 21.97%\n",
            "Epoch 80/500 | Train Loss: 2.6418 | Val Loss: 3.0182 | Train Acc: 35.47% | Val Acc: 22.25%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 81/500 | Train Loss: 2.6301 | Val Loss: 3.0183 | Train Acc: 35.42% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 82/500 | Train Loss: 2.6215 | Val Loss: 3.0163 | Train Acc: 34.89% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 83/500 | Train Loss: 2.5965 | Val Loss: 3.0160 | Train Acc: 36.24% | Val Acc: 22.83%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 84/500 | Train Loss: 2.5522 | Val Loss: 2.9692 | Train Acc: 38.07% | Val Acc: 24.86%\n",
            "Epoch 85/500 | Train Loss: 2.5604 | Val Loss: 3.0043 | Train Acc: 35.89% | Val Acc: 22.25%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 86/500 | Train Loss: 2.5227 | Val Loss: 2.9980 | Train Acc: 38.01% | Val Acc: 23.41%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 87/500 | Train Loss: 2.5214 | Val Loss: 2.9956 | Train Acc: 38.54% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 88/500 | Train Loss: 2.5117 | Val Loss: 2.9306 | Train Acc: 37.89% | Val Acc: 23.70%\n",
            "Epoch 89/500 | Train Loss: 2.5230 | Val Loss: 3.0027 | Train Acc: 38.07% | Val Acc: 21.97%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 90/500 | Train Loss: 2.4674 | Val Loss: 2.9576 | Train Acc: 40.19% | Val Acc: 22.83%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 91/500 | Train Loss: 2.4710 | Val Loss: 2.9092 | Train Acc: 38.89% | Val Acc: 25.43%\n",
            "Epoch 92/500 | Train Loss: 2.4525 | Val Loss: 2.9427 | Train Acc: 40.19% | Val Acc: 23.70%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 93/500 | Train Loss: 2.4495 | Val Loss: 2.9580 | Train Acc: 39.60% | Val Acc: 21.39%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 94/500 | Train Loss: 2.4158 | Val Loss: 2.9510 | Train Acc: 41.07% | Val Acc: 24.28%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 95/500 | Train Loss: 2.3957 | Val Loss: 2.9128 | Train Acc: 42.55% | Val Acc: 23.70%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 96/500 | Train Loss: 2.3789 | Val Loss: 2.8851 | Train Acc: 41.54% | Val Acc: 24.86%\n",
            "Epoch 97/500 | Train Loss: 2.3558 | Val Loss: 2.9379 | Train Acc: 43.43% | Val Acc: 22.54%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 98/500 | Train Loss: 2.3411 | Val Loss: 2.8647 | Train Acc: 44.49% | Val Acc: 24.28%\n",
            "Epoch 99/500 | Train Loss: 2.3344 | Val Loss: 2.8789 | Train Acc: 42.84% | Val Acc: 24.28%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 100/500 | Train Loss: 2.3184 | Val Loss: 2.9141 | Train Acc: 44.55% | Val Acc: 23.12%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 101/500 | Train Loss: 2.3047 | Val Loss: 2.8282 | Train Acc: 43.90% | Val Acc: 24.86%\n",
            "Epoch 102/500 | Train Loss: 2.2997 | Val Loss: 2.8666 | Train Acc: 44.49% | Val Acc: 23.41%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 103/500 | Train Loss: 2.2609 | Val Loss: 2.9013 | Train Acc: 47.67% | Val Acc: 22.83%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 104/500 | Train Loss: 2.2744 | Val Loss: 2.8068 | Train Acc: 45.55% | Val Acc: 26.59%\n",
            "Epoch 105/500 | Train Loss: 2.2527 | Val Loss: 2.8234 | Train Acc: 45.32% | Val Acc: 24.57%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 106/500 | Train Loss: 2.2379 | Val Loss: 2.7365 | Train Acc: 46.32% | Val Acc: 28.61%\n",
            "Epoch 107/500 | Train Loss: 2.2397 | Val Loss: 2.7912 | Train Acc: 46.38% | Val Acc: 24.86%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 108/500 | Train Loss: 2.2186 | Val Loss: 2.7984 | Train Acc: 46.91% | Val Acc: 25.14%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 109/500 | Train Loss: 2.1721 | Val Loss: 2.8080 | Train Acc: 48.20% | Val Acc: 26.01%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 110/500 | Train Loss: 2.1731 | Val Loss: 2.7963 | Train Acc: 50.32% | Val Acc: 27.17%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 111/500 | Train Loss: 2.1656 | Val Loss: 2.8169 | Train Acc: 48.44% | Val Acc: 26.88%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 112/500 | Train Loss: 2.1488 | Val Loss: 2.8300 | Train Acc: 48.50% | Val Acc: 24.86%\n",
            "EarlyStopping counter: 6 out of 75\n",
            "Epoch 113/500 | Train Loss: 2.1199 | Val Loss: 2.7789 | Train Acc: 49.91% | Val Acc: 26.88%\n",
            "EarlyStopping counter: 7 out of 75\n",
            "Epoch 114/500 | Train Loss: 2.1178 | Val Loss: 2.8252 | Train Acc: 51.03% | Val Acc: 23.12%\n",
            "EarlyStopping counter: 8 out of 75\n",
            "Epoch 115/500 | Train Loss: 2.1075 | Val Loss: 2.8212 | Train Acc: 50.68% | Val Acc: 23.12%\n",
            "EarlyStopping counter: 9 out of 75\n",
            "Epoch 116/500 | Train Loss: 2.1022 | Val Loss: 2.7582 | Train Acc: 49.62% | Val Acc: 26.59%\n",
            "EarlyStopping counter: 10 out of 75\n",
            "Epoch 117/500 | Train Loss: 2.0763 | Val Loss: 2.7547 | Train Acc: 51.44% | Val Acc: 25.43%\n",
            "EarlyStopping counter: 11 out of 75\n",
            "Epoch 118/500 | Train Loss: 2.0686 | Val Loss: 2.7131 | Train Acc: 50.56% | Val Acc: 27.46%\n",
            "Epoch 119/500 | Train Loss: 2.0540 | Val Loss: 2.6597 | Train Acc: 52.15% | Val Acc: 27.46%\n",
            "Epoch 120/500 | Train Loss: 2.0515 | Val Loss: 2.7337 | Train Acc: 52.33% | Val Acc: 26.59%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 121/500 | Train Loss: 2.0241 | Val Loss: 2.7562 | Train Acc: 53.68% | Val Acc: 24.28%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 122/500 | Train Loss: 2.0209 | Val Loss: 2.7041 | Train Acc: 51.74% | Val Acc: 28.03%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 123/500 | Train Loss: 2.0019 | Val Loss: 2.7086 | Train Acc: 53.68% | Val Acc: 28.61%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 124/500 | Train Loss: 1.9655 | Val Loss: 2.6844 | Train Acc: 54.33% | Val Acc: 29.19%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 125/500 | Train Loss: 1.9411 | Val Loss: 2.6602 | Train Acc: 54.04% | Val Acc: 28.03%\n",
            "EarlyStopping counter: 6 out of 75\n",
            "Epoch 126/500 | Train Loss: 1.9310 | Val Loss: 2.6035 | Train Acc: 54.92% | Val Acc: 30.64%\n",
            "Epoch 127/500 | Train Loss: 1.9673 | Val Loss: 2.6715 | Train Acc: 55.10% | Val Acc: 29.19%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 128/500 | Train Loss: 1.9260 | Val Loss: 2.6645 | Train Acc: 54.74% | Val Acc: 27.46%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 129/500 | Train Loss: 1.9030 | Val Loss: 2.6522 | Train Acc: 57.28% | Val Acc: 28.61%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 130/500 | Train Loss: 1.8778 | Val Loss: 2.6645 | Train Acc: 56.87% | Val Acc: 30.06%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 131/500 | Train Loss: 1.8802 | Val Loss: 2.6369 | Train Acc: 55.45% | Val Acc: 31.50%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 132/500 | Train Loss: 1.8966 | Val Loss: 2.5728 | Train Acc: 56.75% | Val Acc: 32.08%\n",
            "Epoch 133/500 | Train Loss: 1.8436 | Val Loss: 2.5643 | Train Acc: 57.51% | Val Acc: 31.21%\n",
            "Epoch 134/500 | Train Loss: 1.8419 | Val Loss: 2.5847 | Train Acc: 58.22% | Val Acc: 31.50%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 135/500 | Train Loss: 1.8307 | Val Loss: 2.5976 | Train Acc: 57.22% | Val Acc: 30.64%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 136/500 | Train Loss: 1.8355 | Val Loss: 2.5510 | Train Acc: 57.81% | Val Acc: 33.82%\n",
            "Epoch 137/500 | Train Loss: 1.8054 | Val Loss: 2.6494 | Train Acc: 59.99% | Val Acc: 30.06%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 138/500 | Train Loss: 1.8070 | Val Loss: 2.6408 | Train Acc: 57.45% | Val Acc: 30.64%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 139/500 | Train Loss: 1.7910 | Val Loss: 2.6090 | Train Acc: 59.46% | Val Acc: 32.37%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 140/500 | Train Loss: 1.7676 | Val Loss: 2.6308 | Train Acc: 58.69% | Val Acc: 30.35%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 141/500 | Train Loss: 1.7618 | Val Loss: 2.6058 | Train Acc: 59.69% | Val Acc: 30.64%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 142/500 | Train Loss: 1.7471 | Val Loss: 2.6318 | Train Acc: 60.75% | Val Acc: 30.64%\n",
            "EarlyStopping counter: 6 out of 75\n",
            "Epoch 143/500 | Train Loss: 1.7021 | Val Loss: 2.6159 | Train Acc: 62.05% | Val Acc: 31.79%\n",
            "EarlyStopping counter: 7 out of 75\n",
            "Epoch 144/500 | Train Loss: 1.7055 | Val Loss: 2.6247 | Train Acc: 60.75% | Val Acc: 29.77%\n",
            "EarlyStopping counter: 8 out of 75\n",
            "Epoch 145/500 | Train Loss: 1.6931 | Val Loss: 2.6062 | Train Acc: 61.70% | Val Acc: 31.50%\n",
            "EarlyStopping counter: 9 out of 75\n",
            "Epoch 146/500 | Train Loss: 1.6702 | Val Loss: 2.5670 | Train Acc: 62.64% | Val Acc: 31.79%\n",
            "EarlyStopping counter: 10 out of 75\n",
            "Epoch 147/500 | Train Loss: 1.6424 | Val Loss: 2.5553 | Train Acc: 63.64% | Val Acc: 32.37%\n",
            "EarlyStopping counter: 11 out of 75\n",
            "Epoch 148/500 | Train Loss: 1.6579 | Val Loss: 2.5649 | Train Acc: 63.35% | Val Acc: 33.53%\n",
            "EarlyStopping counter: 12 out of 75\n",
            "Epoch 149/500 | Train Loss: 1.6404 | Val Loss: 2.5869 | Train Acc: 62.88% | Val Acc: 31.79%\n",
            "EarlyStopping counter: 13 out of 75\n",
            "Epoch 150/500 | Train Loss: 1.6351 | Val Loss: 2.4964 | Train Acc: 62.82% | Val Acc: 34.10%\n",
            "Epoch 151/500 | Train Loss: 1.6152 | Val Loss: 2.5375 | Train Acc: 64.23% | Val Acc: 31.50%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 152/500 | Train Loss: 1.6003 | Val Loss: 2.5798 | Train Acc: 65.23% | Val Acc: 31.21%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 153/500 | Train Loss: 1.5873 | Val Loss: 2.5616 | Train Acc: 65.35% | Val Acc: 32.95%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 154/500 | Train Loss: 1.6010 | Val Loss: 2.5559 | Train Acc: 65.06% | Val Acc: 31.50%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 155/500 | Train Loss: 1.5663 | Val Loss: 2.5578 | Train Acc: 66.00% | Val Acc: 33.24%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 156/500 | Train Loss: 1.5570 | Val Loss: 2.5150 | Train Acc: 66.47% | Val Acc: 31.79%\n",
            "EarlyStopping counter: 6 out of 75\n",
            "Epoch 157/500 | Train Loss: 1.5470 | Val Loss: 2.4463 | Train Acc: 66.12% | Val Acc: 36.71%\n",
            "Validation loss decreased (inf --> 2.446337).  Saving model ...\n",
            "Epoch 158/500 | Train Loss: 1.5174 | Val Loss: 2.5048 | Train Acc: 67.12% | Val Acc: 35.84%\n",
            "EarlyStopping counter: 1 out of 75\n",
            "Epoch 159/500 | Train Loss: 1.5011 | Val Loss: 2.5079 | Train Acc: 67.18% | Val Acc: 35.26%\n",
            "EarlyStopping counter: 2 out of 75\n",
            "Epoch 160/500 | Train Loss: 1.5128 | Val Loss: 2.4862 | Train Acc: 66.82% | Val Acc: 35.55%\n",
            "EarlyStopping counter: 3 out of 75\n",
            "Epoch 161/500 | Train Loss: 1.4931 | Val Loss: 2.4910 | Train Acc: 68.47% | Val Acc: 33.24%\n",
            "EarlyStopping counter: 4 out of 75\n",
            "Epoch 162/500 | Train Loss: 1.4865 | Val Loss: 2.4828 | Train Acc: 67.94% | Val Acc: 33.82%\n",
            "EarlyStopping counter: 5 out of 75\n",
            "Epoch 163/500 | Train Loss: 1.4714 | Val Loss: 2.4652 | Train Acc: 66.76% | Val Acc: 35.84%\n",
            "EarlyStopping counter: 6 out of 75\n",
            "Epoch 164/500 | Train Loss: 1.4884 | Val Loss: 2.4912 | Train Acc: 68.18% | Val Acc: 37.86%\n",
            "EarlyStopping counter: 7 out of 75\n",
            "Epoch 165/500 | Train Loss: 1.4385 | Val Loss: 2.5256 | Train Acc: 69.77% | Val Acc: 34.10%\n",
            "EarlyStopping counter: 8 out of 75\n",
            "Epoch 166/500 | Train Loss: 1.4388 | Val Loss: 2.5070 | Train Acc: 69.00% | Val Acc: 34.39%\n",
            "EarlyStopping counter: 9 out of 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Calculate Min/Max Statistics ---\n",
        "min_train_loss = min(train_losses)\n",
        "min_val_loss = min(val_losses)\n",
        "\n",
        "max_train_acc = max(train_accs)\n",
        "max_val_acc = max(val_accs)\n",
        "\n",
        "max_train_top5 = max(train_top5_accs)\n",
        "max_val_top5 = max(val_top5_accs)\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(18, 5)) # Wider figure for 3 plots\n",
        "\n",
        "# 1. Plot Loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(f'Loss\\n(Min Train: {min_train_loss:.2f}, Min Val: {min_val_loss:.2f})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 2. Plot Top-1 Accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(val_accs, label='Val Acc')\n",
        "plt.title(f'Mean Accuracy\\n(Max Train: {max_train_acc:.2f}%, Max Val: {max_val_acc:.2f}%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "# 3. Plot Top-5 Accuracy\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(train_top5_accs, label='Train Acc (Top-5)')\n",
        "plt.plot(val_top5_accs, label='Val Acc (Top-5)')\n",
        "plt.title(f'Top-5 Accuracy\\n(Max Train: {max_train_top5:.2f}%, Max Val: {max_val_top5:.2f}%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pfRPgcTc78O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running final evaluation to generate plots...\")\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_scores = []\n",
        "\n",
        "model.eval() # Ensure model is in eval mode\n",
        "with torch.no_grad():\n",
        "    for seqs, labels, _ in val_loader:\n",
        "        seqs = seqs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(seqs)\n",
        "\n",
        "        # --- For Confusion Matrix ---\n",
        "        # Get the class with the highest score\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # --- For ROC Curve ---\n",
        "        # Get the raw probability scores using softmax\n",
        "        scores = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        all_scores.append(scores.cpu().numpy())\n",
        "\n",
        "        # --- For Both ---\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_labels_np = np.array(all_labels)\n",
        "all_predictions_np = np.array(all_predictions)\n",
        "all_scores_np = np.concatenate(all_scores, axis=0)\n",
        "\n",
        "print(\"Evaluation complete.\")"
      ],
      "metadata": {
        "id": "97OGGQta8CNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Micro-Average ROC Curve ---\n",
        "y_true_binarized = label_binarize(all_labels_np, classes=list(range(NUM_CLASSES)))\n",
        "\n",
        "# Calculate the ROC curve for the \"micro-average\"\n",
        "fpr, tpr, _ = roc_curve(y_true_binarized.ravel(), all_scores_np.ravel())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\"Micro-Average ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot the curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'Micro-average ROC curve (area = {roc_auc:0.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Micro-Average Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ACPzllim8HJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# --- Analyze Top Confusions ---\n",
        "\n",
        "print(\"\\n--- Top 10 Most Confused Words ---\")\n",
        "\n",
        "# 1. Collect all error pairs\n",
        "# We look for instances where the prediction does NOT match the label\n",
        "error_pairs = []\n",
        "\n",
        "for i in range(len(all_labels_np)):\n",
        "    true_idx = all_labels_np[i]\n",
        "    pred_idx = all_predictions_np[i]\n",
        "\n",
        "    if true_idx != pred_idx:\n",
        "        # Convert indices back to words using your map\n",
        "        true_word = index_to_gloss_map[true_idx]\n",
        "        pred_word = index_to_gloss_map[pred_idx]\n",
        "        error_pairs.append((true_word, pred_word))\n",
        "\n",
        "# 2. Count the occurrences of each specific error pair\n",
        "# e.g., ('APPLE', 'PEAR'): 5 times\n",
        "error_counts = Counter(error_pairs)\n",
        "\n",
        "# 3. Get the top 10 most common errors\n",
        "top_10_errors = error_counts.most_common(10)\n",
        "\n",
        "# 4. Print them neatly\n",
        "if not top_10_errors:\n",
        "    print(\"Amazing! No errors found in the validation set.\")\n",
        "else:\n",
        "    print(f\"{'True Word':<20} | {'Predicted As':<20} | {'Count':<5}\")\n",
        "    print(\"-\" * 50)\n",
        "    for (true_word, pred_word), count in top_10_errors:\n",
        "        print(f\"{true_word:<20} | {pred_word:<20} | {count:<5}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Optional: Print Most Difficult Words (Lowest Recall) ---\n",
        "# This finds words the model gets wrong *most often* in general\n",
        "print(\"\\n--- Top 5 'Hardest' Words (Lowest Accuracy) ---\")\n",
        "class_correct = list(0. for i in range(NUM_CLASSES))\n",
        "class_total = list(0. for i in range(NUM_CLASSES))\n",
        "\n",
        "for i in range(len(all_labels_np)):\n",
        "    label = all_labels_np[i]\n",
        "    pred = all_predictions_np[i]\n",
        "    if label == pred:\n",
        "        class_correct[label] += 1\n",
        "    class_total[label] += 1\n",
        "\n",
        "word_accuracies = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    if class_total[i] > 0:\n",
        "        acc = 100 * class_correct[i] / class_total[i]\n",
        "        word_accuracies.append((index_to_gloss_map[i], acc, class_total[i]))\n",
        "\n",
        "# Sort by accuracy (ascending)\n",
        "word_accuracies.sort(key=lambda x: x[1])\n",
        "\n",
        "for word, acc, total in word_accuracies[:5]:\n",
        "    print(f\"Word: {word:<20} | Accuracy: {acc:.1f}% ({int(total)} samples)\")"
      ],
      "metadata": {
        "id": "75Is0WRrYCP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4AA5JP98Evw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}